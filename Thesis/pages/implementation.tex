\chapter{Implementation}
\label{cha:Implementation}

This chapter will discuss implementation details of the project. Instead of explaining all the technical achievements in detail, the most interesting ones will be discussed alongside the respective challenges encountered.

\section{GreenOnion}

\subsubsection{General design}
This section will briefly discuss the overall technical implementation of GreenOnion alongside a deeper discussion into the unique aspect of the tool.

GreenOnion is written in C++. This languages was chosen due to its efficiency bonuses over other alternative languages.
The tool begins by generating a 2048-bit RSA key through GPG\footnote{GNU Privacy Guard}. This key is then used to create a hash of all but the final 512-bit block of the key. The final block of the key is left un-hashed due to the presence of the exponent ($e$). This exponent is incremented to create a new unique key. This is the aspect provides rapid generation of new keys due to the reduction in expensive large prime generation. This prevents issues in such as entropy starvation when generating a large number of valid RSA keys. This produces valid keys but with abnormally large exponents, this was deemed as suitable due to the short term use-case for these keys. 3 bytes are used to represent the exponent giving the potential to create $2^{24} - 1$ extra keys for a single expensive key generation.

All keys plus the intermediate hash are loaded on the GPU via an OpenCL kernel. This kernel is designed to hash the final block, obtain the final fingerprint and checking if the fingerprint is present in the provided list.

\subsubsection{Bloom filter}

Mass checking of fingerprints is the tools main abilities as it allows for millions of keys to be checked with a very small amount of overhead. This is achieved through the use of a bloom filter. A bloom filter is a probabilistic data structure that allows efficient checking for presence of an element in a set. It is effectively a very large array of booleans that state wether an element is present where the position in the array is decided by a pre-defined hashing algorithm. This is repeated a number of times with a number of hashing algorithms ($k$) to populate the array of length ($m$). This then means checking the presence of an element in the set will just means hashing the target and checking the indices returned. This data structure therefore has a complexity of $O(k)$ regardless of the number of elements in the set. This is the only data structure that provides this useful characteristic.

However, due to the random distribution of the hashing algorithms, there is the possibility for collisions and, thus, the possibility of false-positives. The data structure, however, does not produce any false-negatives. This, therefore, makes it fully suited to this use-case as any fingerprints tagged as ``possibility'' being present in the bloom filter can be followed up with a more expensive hash-table check to determine their actual presence. Therefore, if the levels of false-positives are controlled (by altering $k$ and $m$) the tool can search through huge number of potential keys without any decrease in speed.

\begin{figure}[h!]
    \centering
    \input{diagrams/bloom_filter.tex}
    \caption{Bloom filter example}
    \label{fig:bloom}
\end{figure}

Figure \ref{fig:bloom} shows the operation of a simple bloom filter. As it can be seen the element $n$ definitely does not exist in the set due to both array indices not being set. The performance boost when comparing large numbers of keys is substantial when compared to the similar tool Scallion (See Section \ref{sec:greenDesign}.) The results from the comparison will be quantified in Chapter \ref{cha:Experiments}.

\section{First Experiment}
In order to assess and collect data from users for the first experiment (See Section \ref{exp:metric}) Google Forms\footnote{\url{https://www.google.com/forms}} was used to host and design the questionnaire. It provided functionality to easily present questions and record their responses. However, as the design requirement of renewing sample from the matches of each metric was not part of the main applications functionality a solution was required. This was solved by Google's App Script\footnote{\url{https://developers.google.com/apps-script/}} service that allows a user to programmatically interact with the Form. This allowed the re-sampling of matches each time a user submitted a response.

\begin{figure}[!h]
\begin{lstlisting}[frame=single, language=Javascript]
var ALGOS = ["Soundex", "Metaphone", ...];
var ALGO_SIZES = [763777, 412916, ...];

function updateForm(ss, form) {

  // Obtains a list of the items present on the form
  formItems = form.getItems();
  
  // Updates the new values
  var algo_index = 0;
  var scale_index = 0;
  for (var i = 0; i < formItems.length; i++)
  {
    if (formItems[i].getType() == "SCALE")
    {
      var rnd_index = Math.floor(Math.random() * 
      ALGO_SIZES[algo_index]) + 1;    
      
      // Obtains the Google Sheets object
      var sheetname = ALGOS[algo_index];
      var sheet = ss.getSheetByName(sheetname);
      var range = sheet.getRange(rnd_index, 1)
      var values = range.getValues();
           
      // Re-samples from the connected Google Sheets
      _updateScaleTitle(formItems[i], values[0]);
      
      scale_index++;
      
      // Moves on to the next metric when a group size is
      // been completed
      if (scale_index != 0)
      {
        if (scale_index % QUESTION_PER_ALGO == 0) {
            algo_index++;
        }
      }
    }
  }
}
\end{lstlisting}
\caption{Section of code from the First Experiment's Google App Script}
\label{fig:GoogleAppScript}
\end{figure}

Figure \ref{fig:GoogleAppScript} contains an interesting snippet from the script used in the experiment. Each time a user submitted their response the \verb|updateForm()| endpoint was called, that subsequently updated the questions for the next user. This aimed to keep the samplings fair as they are equality likely to appear when attacking the system alongside an ample exploration of the overall match quality.

\section{MainExperiment}
In order to asses users in a similar scenario to ones experienced when utilizing \pep in the real-world a custom application was required. The front-end is developed in Javascript with a Python-Flask backend that acts as the webpages functionality. A number of end points are exposed like \verb|get_audio| or \verb|get_words| that are requested and displayed on the front-end.

One of the main problems encountered was the handling of each user's session. Initially during testing it was noted that changes in one end-point if timed correctly would overwrite the progress of another, this is commonly know as a ``Lost update''. This was due to the user session being stored in the \verb|session| cookie. When sending off a request the session cookie is included in the header, the code will then utilise the data within the cookie to make decision and alter state, the updated state is then returned as a cookie in the response. If, however, another request is executed before the first has completed, the second request is using the context of the first request and, thus, any alterations made by the first will be completely lost.

\begin{figure}[!h]
  \centering
  \begin{sequencediagram}
    \newthread{A}{Client}{}
    \newinst[2]{B}{get\_audio()}{}
    \newinst[2]{C}{get\_words()}{}
    
    \begin{call}{A}{}{B}{}
    \begin{call}[1]{A}{}{C}{}
    \end{call}
    \end{call}
    
    \node[anchor=east] (c0) at (0.75, -1.325) {$c$};
    \node[below of=c0, yshift=.4cm] (c1) {$c$};
    \node[below of=c1, yshift=.4cm] (c2) {\textbf{\st{$c_{1}$}}};
    \node[below of=c2, yshift=.4cm] (c3) {$c_2$};
  \end{sequencediagram}
  \caption{Sequence diagram for the `Lost Update' problem}
  \label{fig:cookie}
\end{figure}

Figure \ref{fig:cookie} shows this problem visually. As it can be seen the response cookie $c_2$ is based of the old data of $c$ and, therefore, when returned will remove the progress of $c_1$.

To keep the complexity of the backend low it was opted to solve this problem by restricting the user via the UI, when a button was clicked all other operations were disabled until a response was received. This stopped the possibility of sending off another request while another is still processing.

I order to collate data for results later in the process state had to be saved. One potential solution would be to format the data and save it as a .csv file. However, this is a static way of data persistence and would require alteration every time the data schema was modified. The chosen solution utilized a python object serialization library known as `Pickle'\footnote{\url{https://docs.python.org/2/library/pickle.html}}. This library saves the an instance of a class as a file. This file then can be used to load the instance of the class back into the program. This allows changes to the object to filter down through the program and keeps the design of the application fluid. For example, after initial feedback gathered about the application it was decided that a inclusion of `Round times' would be a useful addition. Edits were made to the \verb|experiment| class and the changes propagated through the application, no changes were needed on the save functionality. To distinguish between old and new save formats a version number was used.