\chapter{Implementation}
\label{cha:Implementation}

This chapter discusses the implementation details of the project. Instead of explaining all the technical achievements in detail, the most interesting ones are discussed alongside the respective challenges encountered.

\section{GreenOnion}

\subsubsection{General design}
This section briefly discusses the overall technical implementation of GreenOnion alongside a more in-depth discussion into the unique aspect of the tool.

GreenOnion is written in C++. This language was chosen due to its efficiency bonuses over other alternative languages.
The tool begins by generating a 2048-bit RSA key through GPG\footnote{GNU Privacy Guard}. This key is then used to create a hash of all but the final 512-bit block of the key. The final block of the key is left un-hashed due to the presence of the exponent ($e$). This exponent is then incremented to create a new unique key. This technique allows the accelerated generation of new keys due to the reduction in expensive large prime generation. Entropy starvation is an example of a common issue encountered when generating a large number of RSA keys. This process produces valid keys, but with abnormally large exponents, this was deemed suitable due to the short term use-case for these keys. Three bytes are used to represent the exponent giving the potential to create $2^{24} - 1$ extra keys for a single expensive key generation.

The intermediate hash is then loaded on the GPU via an OpenCL kernel. This kernel is designed to increment the exponent, hash the final block, obtain the final fingerprint and check if the fingerprint is present in the provided data structure.

\subsubsection{Bloom filter}

Mass checking of fingerprints is the tool's central ability as it allows for millions of keys to be checked with a minimal amount of overhead. This functionality is achieved through the use of a bloom filter. A bloom filter is a probabilistic data structure that allows efficient checking for the presence of an element in a set. It is effectively a vast array of booleans that state whether an element is present. A pre-defined hashing algorithm decides the index in the array. This process is repeated several times with a set number of hashing algorithms ($k$) to populate the array of length ($m$). To check the presence of an element in the set means hashing the target value and checking the indices returned. This data structure, therefore, has a complexity of $O(k)$ regardless of the number of elements in the set. This is the only data structure that provides this characteristic. This benefit comes at a cost. Due to the use of hashing algorithms, there is the possibility for collisions and, thus, the possibility of false-positives. The data structure, however, does not produce any false-negatives. 

These qualities, therefore, makes it fully suited to this use-case as any fingerprints tagged as ``possibly'' being present in the bloom filter can be followed up with a more expensive hash-table check to determine their actual presence. Therefore, if the levels of false-positives are controlled (by altering $k$ and $m$), the tool can search through a vast number of potential keys without any decrease in speed.

\begin{figure}[h!]
    \centering
    \input{diagrams/bloom_filter.tex}
    \caption{Bloom filter example}
    \label{fig:bloom}
\end{figure}

Figure \ref{fig:bloom} shows the operation of a simple bloom filter. As can be seen, the element $n$ does not exist in the set due to both array indices not being set. The performance boost when comparing large numbers of keys is substantial when compared to a similar tool Scallion (See Section \ref{sec:greenDesign}.) The results from the comparison are quantified in Chapter \ref{cha:Experiments}.

\subsubsection*{OpenCL}

In order to process a substantial number of keys in a concurrent fashion, OpenCL was required. Figure \ref{fig:opencl} contains the main sections of the code uploaded to the GPU.

\begin{figure}[!h]
  \centering
  \input{code/OpenKernel.tex}
  \caption{OpenCL Kernel}
  \label{fig:opencl}
\end{figure}

The \verb|<EXPONENT_INCREMENTING_CODE>| is explained in the next section. After the exponent is incremented, the code produces the final digest by calling \verb|sha1_block(W, H)|. The 32-bit left and right sections of the digest are hashed using the MurmurHash algorithm. MurmurHash does not exhibit features suitable for secure hashing as it can be trivially reversed. Its use-cases include hash-based lookups due to its speed and its distributive properties, making it suitable to work well with a bloom filter. The hashed values are then checked against the bloom filter array. If there is a possible match, the identification value (\verb|0x12345678|) is placed in the first byte of the \verb|outResult| alongside the exponent used to produce the possibly matching PGP key. The minimal amount of data returned to the CPU via the \verb|outResult| variable is due to efficiency reasons. Use of this memory is computationally expensive; therefore, at every opportunity, writing or reading this memory has been avoided.

\begin{figure}[!h]
  \centering
  \input{code/ExponentIncrement.tex}
  \caption{Exponent incrementation code}
  \label{fig:increment}
\end{figure}

Figure \ref{fig:increment} contains the code used to increment the exponent. Due to the exponent being split across two words alongside the limited functionality of OpenCL, bitwise operations are used to increment the exponent. The code utilises the fact that anything XORed with itself becomes zero alongside the fact that XORing a value $a$ with zero provides the result equal to $a$. The code manipulates the position of values by multiplication or division  of base-16 values; these shift the hex value either left or right, respectively. Once it is in position the "Mask" is created by XORing with the original value and the new intended value, this stores the changes required to reach the required values, this then applied to each of the words.
The \verb|get_global_id| command takes the ID from the currently running thread; this allows a different value to be used for each exponent concurrently.

\newpage

\section{First Experiment}
\label{sec:exp1_implemtation}
As discussed in Section \ref{sec:matches}, a list of matches were generated for each metric. Figure \ref{fig:trustwordMatch} shows the algorithm designed to compute these lists. It loads the entire Trustword dictionary and works through all combinations of words.

\begin{figure}[h!]
  \centering
  \input{code/TrustwordPlusPlus.tex}
  \caption{Main code used to generate match list}
  \label{fig:trustwordMatch}
\end{figure}

Figure \ref{fig:matchExample} further shows the code used in the \verb|leven_similar|. The Levenshtein difference is computed and compared to the tolerance discussed in Section \ref{sec:matches}. If this function returns true the match is added to the list.

\begin{figure}[h!]
\begin{Code}[CppStyle]
bbool leven_similar(string word1, string word2)
{
  int diff = lev_distance(word1, word2);
  return diff <= DIFFERENCE_TOLERANCE;
}
\end{Code}
\caption{Example similar match function}
\label{fig:matchExample}
\end{figure}

\newpage

In order to assess and collect data from users for the first experiment (See Section \ref{exp:metric}), Google Forms\footnote{\url{https://www.google.com/forms}} was used to host and design the questionnaire. It provided the functionality to present questions easily and record responses. However, as the design requirement of refreshing questions after each submission was not part of the main applications functionality, an extra solution was required. This problem was solved by Google's App Script\footnote{\url{https://developers.google.com/apps-script/}} service that allows a user to interact with the Form programmatically. This addition allowed the re-sampling of matches  each time a response is submitted. 

\begin{figure}[!h]
  \centering
  \input{code/GoogleAppScript.tex}
\caption{Section of code from the First Experiment's Google App Script}
\label{fig:GoogleAppScript}
\end{figure}

Figure \ref{fig:GoogleAppScript} contains an interesting snippet from the script used in the experiment. Each time a user submitted their response, the \verb|updateForm()| endpoint was called, that subsequently updated the questions for the next user. This technique aimed to keep the samplings fair as they are equally likely to appear when attacking the system.

\newpage

\section{MainExperiment}
In order to assess users in a similar scenario to ones experienced when utilizing \pep in the real-world, a custom application was required. The front-end is developed in Javascript with a Python-Flask backend that acts as the functionality of the webpage. Several endpoints are exposed like \verb|get_audio| or \verb|get_words| that are requested and displayed on the front-end.

One of the main problems encountered was the handling of each user's session. Initially, during testing, it was noted that changes in one endpoint if timed correctly would overwrite the progress of another, this is commonly known as a ``Lost update''. This issue was due to the user session being stored in the \verb|session| cookie. When sending off a request the session cookie is included in the header, the code then utilises the data within the cookie to make decisions and alter state, the updated state is then returned as a cookie in the response. If, however, another request is executed before the first has completed, the second request is using the context of the first request and, thus, any alterations made by the first is completely lost.

\begin{figure}[!h]
  \centering
  \begin{sequencediagram}
    \newthread{A}{Client}{}
    \newinst[2]{B}{get\_audio()}{}
    \newinst[2]{C}{get\_words()}{}
    
    \begin{call}{A}{}{B}{}
    \begin{call}[1]{A}{}{C}{}
    \end{call}
    \end{call}
    
    \node[anchor=east] (c0) at (0.75, -1.325) {$c$};
    \node[below of=c0, yshift=.4cm] (c1) {$c$};
    \node[below of=c1, yshift=.4cm] (c2) {\textbf{\st{$c_{1}$}}};
    \node[below of=c2, yshift=.4cm] (c3) {$c_2$};
  \end{sequencediagram}
  \caption{Sequence diagram for the `Lost Update' problem}
  \label{fig:cookie}
\end{figure}

Figure \ref{fig:cookie} shows this problem visually. As it can be seen the response cookie $c_2$ is based on the old data of $c$. When $c_2$ is eventually returned, it removes the progress of $c_1$. The restriction of the UI solved this problem. When a button is clicked, all other operations are disabled until a response is received. This technique stopped the possibility of sending off another request while another is still processing. This solution was chosen to keep the complexity of the backend low.

In order to collate data for results later in the process, the state had to be retained. One potential solution would be to format the data and save it as a \verb|.csv| file. However, this is a static way of data persistence and would require alteration every time the data schema was modified. The chosen solution utilised a python object serialisation library known as `Pickle'\footnote{\url{https://docs.python.org/2/library/pickle.html}}. This library saves the instance of a class as a file. This file then can be used to load the instance of the class back into the program. This inclusion allows changes to the object to filter down through the program and keeps the design of the application fluid. For example, after initial feedback gathered about the application, it was decided that the inclusion of `Round times' would be a useful addition. Edits were made to the \verb|experiment| class and the changes propagated through the application; no changes were needed on the save functionality. To distinguish between old and new save formats, a version number was used.