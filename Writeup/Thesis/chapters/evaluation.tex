\chapter{Conclusions}
This chapter collates the findings of the paper, evaluates the paper's success against the research questions and discuss potential further work.

\section{Research Question \ref{goal:phoneticSimilarity}}

Research Question 1 asks: \textit{“What are the best performing schemes to quantify phonetic similarity?”}. This paper has discussed several metrics used to quantify phonetic similarity (See Section \ref{sec:metrics}). Most metrics were chosen due to their high utilization and occurrence in literature. The only exception to this is the presence of Phonetic Vectors that were chosen due to their unique qualities. Performance of these metrics were then quantified in a direct study that selected the top three from a set of five (See Section \ref{exp:metric}). These newly selected metrics were then assessed in an attack context with their performance being indirectly quantified (See Section \ref{sec:exp2_design}).

The top three metrics (in descending order) were: Levenshtein, NYSIIS and Metaphone. We found that Levenshtein was the most aligned with how people rate phonetically similarity. Levenshtein had a considerable proportion of 4 and 5 ratings (64\%) compared to the alternatives. Levenshtein was also the best performer in the attack context with it having the best attack success rate in all attacks. This corroborates work showing that words with similar spelling tend to share phonetic similarity \cite{hettiarachchi2012sparcl}.
Alongside this, we found Soundex was the least aligned with the human model of similarity with an average rating of just 2 out of 5. This corroborates findings that highlighted major issues with the accuracy of Soundex’s phonetic representation (See Section \ref{sec:soundex}).

We believe the outcome of the first experiment were inflated by the way the results were presented (as discussed in Section \ref{sec:exp1_considerations}). However, as the performance of the metrics in the second experiment emulated the first with Levenshtein being the best performer alongside Metaphone being the worst, shows that the results of the first study maintains a desirable level of accuracy. Improvements can be made with the proposed design discussed in Section \ref{sec:exp1_considerations}, but, we believe the first experiment has achieved its goal of providing a low cost way to reduce the number of selected metrics for the subsequent study.

\section{Research Question \ref{goal:complexity}}
Research Question \ref{goal:complexity}: \textit{``What are the time and computation complexity required to generate a ’similar’ keys for a targeted key pair?''} has been discussed by the project, the actual generation of keys as shown in Section \ref{sec:key_gen} where actual keys were generated. Furthermore, the distribution of key permutations on a set of real-world keys were discussed in Section \ref{sec:averagePerms} and \ref{sec:vulnKeys}. To achieve this, the tool GreenOnion was developed where inspiration was gathered from a currently available tool known as Scallion. Experimentation results were gathered and displayed in Section \ref{sec:SvG} that showed the significant increase in the number of near-collision keys GreenOnion can test for with GreenOnion being able to handle in excess of 1.5 million keys. This capability is in comparison to the maximum of 2513 keys Scallion can compare. We believe this is due to the implementation of the bloom filter. Its unique characteristics of $O(k)$ complexity regardless of number of elements in the structure allows a huge number of concurrent checks to occur. 

\section{Research Question \ref{goal:attackPercentage}}
Research Question \ref{goal:attackPercentage} \textit{``What percentage of attacks successfully deceive a user?''} was shown by the results presented in Section \ref{sec:exp2}. The weakest attack strength and ones that can be computed by a single mid-range GPU for a single week achieved success rate ranging from 9.35\% - 11.01\%. The middle attack involving the use of 10 mid-range GPUs for a week had success rates of 15.20\% - 19.20\%. Finally, the highest attack strength that theoretically involved the use of 100 mid-range GPUs computing for a week has success rates: 24.43\% - 32.05\%. Therefore, showing the number of attacks would deceive a set of users.

\section{Research Question \ref{goal:numberOfTrustwords}}
Research Question \ref{goal:numberOfTrustwords} states \textit{`` Is the recommended minimum number of four Trustwords enough to provide a basic level of security?''}. The paper has investigated this through the creation of an attack and quantification of its success on actual participants. Findings have shown that an attack computed in half a day can have a success rate of about 9.35\%. A user, therefore, could initiate this attack on-mass with hardware present inside the average gaming computer. On the other hand, the best level of attack success was shown to be 32.05\%. 

Furthermore, comparison of these results to similar literature shows a considerable increase in success rates for the attack present in this paper. We believe this is due to the consideration of `similarity' when forming attacks. This was an aspect missing from the literature.
The effect of `similarity' is further enhanced due to the  weaknesses present in Trustwords such as homophones, dual mappings and the wordlist size. Therefore, we believe that 4 Trustwords as a minimum has not provided a basic level of security towards users.

\section{Further work}

\subsection*{Trustword improvements}
The first area of proposed work could be recommendations into how to improve Trustwords backed by empirical evidence. We feel the most promising avenue would be the utilization of Phonetic Vectors unique qualities to identify words of low quality by measuring very low vector distances. For example, present in the dictionary are the words \verb|THERE| and \verb|THEIR|, these could be massively improved upon and could result in a better quality word list. Moreover, utilizing the quantification of dissimilarity could be used to create a dictionary of maximized phonetic distance that could then be assessed in a similar way to detect the feasibility of the attack with the alterations and its success rate against users.

\subsection*{Similar metrics performance}
Another area of further work is the comprehensive assessment of algorithms used to assess phonetic similarity. The algorithms assessed in this work are a very small proportion of potential options. Thus, further work could assess the performance of metrics against each other to determine the one that works best with human models of phonetics. A conclusion could be reached by running the improved experiment discussed in Section \ref{exp:metric}, where it could be repeated with an increased of metrics. Other elements for consideration could be age, location and dialect as variables that affect the metrics performance. 

\subsection*{GreenOnion optimizations}
A minimal amount of work and time was allocated to improving the performance of GreenOnion as low-level optimizations are very time-consuming. A project, therefore, could aim to improve on the performance already recorded in this paper. This project could, therefore, improve the performance of the attack and allow for more effective keys to be computed in less time. 

\section{Final Remarks}
Overall, the project has demonstrated a potential attack on \pep's implementation of Trustwords. An attack was proposed that utilised phonetic similarity algorithms to exploit the weaknesses in the Trustword dictionary to generate near-collision keys. To achieve this, a tool was created to compute a massive number of keys concurrently. It was based on a known tool but improved substantially on its key searching performance. The performance of similarity metrics was compared alongside an experiment assessing participants fallibility to the proposed attack. Results showed as much as a 32.05\% success rate for the best attack. The main project aim was to show that the recommend minimum number of four Trustwords was insufficient to provide a basic level of security. We believe we have demonstrated that four Trustwords is too low to provide enough security for general use-case and the minimum provided words should, therefore, be increased.